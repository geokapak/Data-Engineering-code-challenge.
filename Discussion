Firstly, it can be observed that the data has possibly been transferred from multiple datasets. The only significant issue is that there were numerous missing values in the column identifying the day the deal took place or ended, which made it unnecessary to fill the Nans using cluster methods or any other data manipulation. Additionally, some values in the new_values and old_values columns have been removed as their number was insignificant. These values could be missing due to human error, bad data transfer, or the absence of new deals for old companies or old clients which maybe have stop dealing, and in the old deals columns, for new companies and clients respectively.

Furthermore, after creating the tables in SQL and transforming them, the query I created shows the accuracy of the "total activities" field, which counts the activities performed per case using the deal_activities_sample dataset, has a value of 70.9%, which can be considered a decent result for the accuracy of the data, but not perfect.

In the Python code, after running it you can find all the necessary results printed (The average amount of undates made to each deal and the total number of call and email activities). You will also find a code part that loads the new CSV files created to the database created with SQL. Although I could have optimized this procedure by loading them directly from the data frames created to SQL and skipping the step where I had to create an actual CSV, unfortunately, I tried to complete the project in a limited amount of time due to my exam period and having many projects to work on simultaneously.

You can also see that I have created only one SQL query for the same reason. Although I do not think that optimizing that, would be a difficult task.

Regarding the optional request, you can find a python code which is creating a new dataframe, which can be also a csv file, or a query with a new table in sql that has two columns: The inactive ids and the ids with deals took place the last two years.
